# ================== Imports ==================
import pandas as pd
import matplotlib.pyplot as plt
import seaborn as sns
import joblib, json, logging
from sklearn.preprocessing import StandardScaler, PolynomialFeatures
from sklearn.pipeline import Pipeline
from sklearn.linear_model import Ridge
from sklearn.metrics import r2_score, mean_absolute_error
from sklearn.model_selection import train_test_split

logging.basicConfig(level=logging.INFO, format="%(levelname)s: %(message)s")

# ================== Utilities ==================
def save_object(obj, path):
    """Save object to file"""
    joblib.dump(obj, path)
    logging.info(f"Saved object to {path}")

def load_object(path):
    """Load object from file"""
    return joblib.load(path)

# ================== Preprocessing ==================
def preprocess_data(df):
    """Clean, create datetime, scale numeric features"""

    # year fix
    if df["yr"].max() <= 1:
        df["yr"] = 2011 + df["yr"]

    # datetime banani hai
    if "dteday" in df.columns:
        df["datetime"] = pd.to_datetime(
            df["dteday"] + " " + df["hr"].astype(str).str.zfill(2) + ":00:00"
        )
    else:
        df["datetime"] = pd.to_datetime(
            df["yr"].astype(str) + "-" +
            df["mnth"].astype(str).str.zfill(2) + "-01 " +
            df["hr"].astype(str).str.zfill(2) + ":00:00"
        )

    # set datetime index
    df = df.sort_values("datetime").set_index("datetime")

    # scale numeric features
    scaler = StandardScaler()
    num_cols = ["temp", "atemp", "hum", "windspeed"]
    df[num_cols] = scaler.fit_transform(df[num_cols])

    return df, scaler

# ================== Utilities ==================
def save_object(obj, path):
    """Save object to file"""
    joblib.dump(obj, path)
    logging.info(f"Saved object to {path}")

def load_object(path):
    """Load object from file"""
    return joblib.load(path)


# ================== Modeling ==================
def train_model(X_train, y_train, degree=2, alpha=1.0):
    model = Pipeline([
        ("poly", PolynomialFeatures(degree=degree, include_bias=False)),
        ("ridge", Ridge(alpha=alpha, random_state=42))
    ])
    model.fit(X_train, y_train)
    return model

def evaluate_model(model, X, y):
    y_pred = model.predict(X)
    return r2_score(y, y_pred), mean_absolute_error(y, y_pred)

# ================== Main Script ==================
def main():
    # 1. Load data
    df = pd.read_csv("BikeRentalData.csv")

    # check cnt column
    if "cnt" not in df.columns:
        raise ValueError("Dataset must have a 'cnt' column as target variable")

    df, scaler = preprocess_data(df)
    logging.info("Data loaded & preprocessed")

    # 2. Visuals
    visuals(df)

    # 3. Split
    X = df.drop(["cnt", "casual", "registered"], axis=1, errors="ignore")
    y = df["cnt"]
    X_train, X_test, y_train, y_test = train_test_split(
        X, y, test_size=0.2, random_state=42
    )

    # 4. Train
    model = train_model(X_train, y_train, degree=2, alpha=1.0)
    logging.info("Model trained")

    # 5. Evaluate
    r2_train, mae_train = evaluate_model(model, X_train, y_train)
    r2_test, mae_test = evaluate_model(model, X_test, y_test)
    logging.info(f"Train R²: {r2_train:.4f} | Train MAE: {mae_train:.2f}")
    logging.info(f" Test R²: {r2_test:.4f} |  Test MAE: {mae_test:.2f}")

    # 6. Save artifacts
    save_object(model, "trained_model.pkl")
    save_object(scaler, "scaler.pkl")
    with open("feature_names.json", "w") as f:
        json.dump(list(X.columns), f)

if __name__ == "__main__":
    main()

